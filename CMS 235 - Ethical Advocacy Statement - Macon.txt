At time of writing there has been a revolution in the tools that are available to professional developers. Beyond what has been called narrow application AI models like ChatGPT and other chatbots there are even more narrowly focused tools and models that live directly on top of and are intimately familiar with a developer’s codebase. These tools have made it so that with one or two keystrokes and working primarily in natural language a developer can produce entire web pages, complex applications, even applications that will interact with and transmit personal information of end users. Naturally this has raised ethical concerns both internal and external to the application. The question becomes: should AI tooling be restricted to only senior developers?
        For developers the process of interacting with these AIs can take one of two forms. Working in natural language to generate code that can be used–directly by copy and paste if the developer so chooses–within the larger application code base or working with a “co-pilot style AI” that’s reading and making suggestions directly in the codebase. Both approaches, no matter the context, offer developers tools that can offload the cognitive labor of hand-writing code.
With such ease of access for developers there is a real temptation to accept the generated code without engaging with it. However, this direct and unchecked use raises several questions. At a very basic level the ease with which the code is produced reduces the amount of time the developer spends thinking about the code and its intricacies. Skill based jobs require the continuous honing of one’s relevant skillset through practice and active engagement. In his essay, “Is There a Need for a New Technology Ethic?”, David McLean, writing on the ease with which genetic augmentation technology could augment human ability, poses a relevant question:
What would be lost were such “perfections” simply handed to us through a process of gene manipulation rather than through striving? What would happen to human character? (Boylan, Teays, & McLean, 2022, p. 81)


Offloading the work of creating code slows the flywheel of a developer’s skill much in the same way genetic manipulation would slow the development of human character. Moreover, the AI could be considered to be robbing younger developers of sovereignty over their skill development by presenting them with solutions that are biased in favor of the approaches the developers of the model selected for training on instead of encouraging exploration of several competing approaches. As well, past just the ethical implications for the developers building the application are concerns more relevant to the other parties involved in the application they’re building.
In most cases the applications developers are working on are not for their own personal use. Often these applications will be built for other clients with developers simply constructing the app to be handed off entirely. Using AI tooling is always a two way street in that a developer has to describe a scenario to narrow chatbot based tools. Even with the most obfuscated of prompts a developer is still transmitting what amounts to be the intellectual property of another person to a platform. Again, we come to ethical issues of agency. Does the client want an AI tool to have access to the code for their project? This question becomes more pressing in more narrow direct integrations of co-pilot style tools that can directly read and interpret the application code. Companies and individuals must be afforded the right to exercise at least some degree of poetic control over their software. In the case of a two party relationship the developing party must be proactive in reinforcing the poetic agency of the receiving client by interrogating the use of AI tools at the outset of the project. This proactive approach safeguards not only the intellectual property of the company as well as that of the application end-users.
Developers are required to engage in best security practices when building applications that interact with the personal data of end-users. These best practices are born of experience building applications. Ethically speaking these best practices ensure trust in the application can be conferred from developer to owner and from owner to end-user. This chain of trust allows users to confidently use and share their data with the application while maintaining agency over that shared data. When a developer uses AI tooling there’s an increased risk of parts of the application becoming a black box where user data is handled in unknown ways thus breaking the chain of trust for all parties. A developer could address these generated areas to ensure the proper handling of data but that’s only achieved through the lens of experience.
In sum, the explosion in use of AI tooling by developers has brought with it serious ethical concerns that need to be addressed. From a dulling of a developer’s skill, to intellectual property concerns, and the more serious breaches of trust introduced by including black boxes of generated code in an application it becomes clear the experience must drive the use of AI tools in software development. Only by way of a nuanced understanding of the generated code can a developer successfully navigate the ethical landscape formed by the tools at play. Much like the power tool in the woodshop or the 18-wheeler on the highway these tools must be driven with experienced hands. Serious ethical implications lay in wait for those who don’t handle powerful tools with care.